---
title: "Homework 3 of CS 422: Section 04"
author: "Diego Martin Crespo, A20432558"
output: 
  html_notebook:
    toc: true
    toc_float:
        smoth_scrooll: true
---

## Problem 1: Clustering

#### a) Data Cleanup

##### i. Attributes you would remove:
I wouldn't use the attribute Name, since there are 66 different Names in this database. So it wouldn't make sense to change them into factors and use them for clustering

##### ii. Does the data need to be standardize?
No, because the range for all attributes is the same (0-5)

##### iii. Data cleaning
I used excel to clean the data and I've created a file called "file19.csv"

### b) Clustering
#### i. Number of clusters needed
```{r}

m<-read.csv("file19_clean.csv", header=TRUE, sep=',', row.names = 1, stringsAsFactors = FALSE)
#mammals<- scale(m)
#mammals<-as.data.frame(mammals)
mammals<-m
library('factoextra')
library(cluster)
library(ggplot2)
library(cluster)
library(data.table)
library(dplyr)
fviz_nbclust(mammals, kmeans, method="wss")
fviz_nbclust(mammals, kmeans, method="silhouette")

```
The WSS method suggest using between 5 and 7 clusters, as those values are the smallest at the elbow of the graph. The silhoutte graph suggest taking beteen 7 and 10 clusters (they have the highest values and they are really close betbeen them, value 10 it is the highest). Taking into acount both methods and computational cost, it has been decided to use 7 clusters.

#### ii. Create the clusters
```{r}
k<-kmeans(mammals, centers=7, nstart=25)
fviz_cluster(k, data=mammals, main="Clusters")
```
#### iii. How many observations are in each cluster?
```{r}
i=1
while(i<8){
  cat("The size of cluster", i, "is", k$size[i], "\n")
  i<-i+1
}
```
#### iv. What is the total SSE of the clusters?
```{r}
cat("The total SSE is", k$totss)
```
#### v. What is the SSE of each cluster?
```{r}
i=1
while(i<8){
  cat("The SSE of cluster", i, "is", k$withinss[i], "\n")
  i<-i+1
}
```
#### vi. How are the mammals grouped in each cluster
```{r}
mammals$cluster<-0
for(i in 1:7){
 mammals$cluster[which(k$cluster==i)]<-i
}
summary(mammals)
```
```{r}
for(i in 1:7){
  cluster<- mammals[ which(mammals$cluster==i),]
  print(cluster[0])
}
```

If we observe the names of the animals in each cluster, we see that the classification makes sense. It's grouping animals that are similar, that eat similar food according their tooth pattern.

## Problem 2: Hierarchical clustering
First we set the seeds
```{r}
set.seed(1122)
animals35 <- sample_n(tbl = m[,1:8], size = 35)
```
### a) Run Hierarchical clustering

```{r}
l.single<-eclust(animals35, "hclust", hc_method="single")
fviz_dend(l.single, as.ggplot=T, cex = 0.3, main = "Cluster Dendrogram - Single", xlab = "Animals")
```

```{r}
l.complete<-eclust(animals35, "hclust", hc_method="complete")
fviz_dend(l.complete, show_labels=TRUE, as.ggplot=, cex = 0.3, main = "Cluster Dendrogram - Complete", xlab = "Animals")
```

```{r}
l.average<-eclust(animals35, "hclust", hc_method="average")
fviz_dend(l.average, show_labels=TRUE, as.ggplot=T, cex = 0.3, main = "Cluster Dendrogram - Average", xlab = "Animals")


```

### b) Examine each graph produced in (a) and understand the dendrogram

#### In the case of single linkage: the two-singleton are {Grounding, Prairie Dog}, {Eik, Reindeer}, {Ocelot, Jaguar}, {Badger, Shunk}, {Silver hair bat, Lump nose bat} (total of 5)

#### For complete linkage we get: {Grounding, Prairie Dog}, {Sea Lion, Elephant seal}, {Ocelot, Jaguar}, {Eik, Reindeer}, {Badger, Shunk}, {Raccoon, Star nose mole}, {Silver hair bat, Lump nose bat}, {Hoary bat, Pygmy bat} (total of 8)

#### For average linkage: {Grounding, Prairie Dog},  {Ocelot, Jaguar}, {Sea Lion, Elephant seal}, {Badger, Shunk}, {Eik, Reindeer}, {Silver hair bat, Lump nose bat}, {Hoary bat, Pygmy bat} (total of 8)


### c) Of the linkage methods you examined in (b), which linkage method would be considered pure by our definition?
Single method will be "pure" by definition as it has the lowest number of two-singletons equal to 7.

### d) Using the graph corresponding to the linkage method you chose in (d), at at a height of about 125, how many clusters would you have?
```{r}
fviz_dend(l.single, show_labels=TRUE, as.ggplot=T, cex = 0.3, main = "Cluster Dendrogram - Single", xlab = "Animals") + geom_hline(yintercept = 2, linetype = 2)

```
```{r}
cluster<-cutree(l.single, h=2)
table(cluster)
```
We get 7 clusters, above we can see the number of observations that each cluster has.

### e) Now, using the number of clusters you picked in (d), re-run the hierarchical clustering using the three linkage modes
```{r}
single2<-factoextra::eclust(animals35, "hclust", k=5, hc_method="single")
fviz_dend(single2, show_labels=TRUE, as.ggplot=T , cex = 0.3, main = "Cluster Dendrogram - Single2", xlab = "Animals")
```

```{r}
complete2<-factoextra::eclust(animals35, "hclust", k=5, hc_method="complete")
fviz_dend(complete2, show_labels=TRUE, as.ggplot=T, cex = 0.3, main = "Cluster Dendrogram - Complete2", xlab = "Animals")
```

```{r}
average2<-factoextra::eclust(animals35, "hclust", k=5, hc_method="average")
fviz_dend(average2, show_labels=TRUE, as.ggplot=T , cex = 0.3, main = "Cluster Dendrogram - Average2", xlab = "Animals")
```
### f) For each cluster obtained by the value of k used in (e), print the Dunn and Silhouette width
```{r}
library(fpc)
stats_single<-cluster.stats(dist(animals35), single2$cluster)
cat('The dunn index for the Single linkage is:', stats_single$dunn)
cat('\nThe Silhoutte width for the Single linkage is:', stats_single$avg.silwidth)
stats_complete<-cluster.stats(dist(animals35), complete2$cluster)
cat('\n\nThe dunn index for the Complete linkage is:', stats_complete$dunn)
cat('\nThe Silhoutte width for the Complete linkage is:', stats_complete$avg.silwidth)
stats_avg<-cluster.stats(dist(animals35), average2$cluster)
cat('\n\nThe dunn index for the Average linkage is:', stats_avg$dunn)
cat('\nThe Silhoutte width for the Average linkage is:', stats_avg$avg.silwidth)
```
#### g) From the three clusters in (f), which is the best cluster obtained if you consider the Dunn index and Silhoutte?
For Dunn consideration: Single is the best one since the Dunn value is higher.
For Silhoutte consideration: Single is better since it has a higher Silhoutte value.


## Problem 3: K-Means and PCA
### a) Perform PCA on the dataset
```{r}
data <- read.csv('HTRU_2-small.csv', sep=',' , header=TRUE)
#we check that there is no missing values:
stopifnot(sum(complete.cases(data))==dim(data)[1])
#before we do the analysis we have to remove the last column, since it is the answer we are looking for and it would be overfitting
df<-data[, -9]
pca<-prcomp(scale(df))
```

#### i. How much cumulative variance is explained by the first two components?
```{r}
summary(pca)
```
The first two components explain 0.7854 of the variance

#### ii. Plot the first two principal components
```{r}
library(ggfortify)
autoplot(pca, data=data, colour='class', loadings=TRUE, loadings.label=TRUE)
```

#### iii. Describe what you see with respect to the actual label of the HTRU2 dataset
The standard deviation and the mean for both the integrated profile and the DM-SNR curve seem to go together and they are opposite of the skewness and the kurtosis in both cases. They also seem to have the same gradient, more or less. 

#### b)

#### i. Perform K-means clustering on the dataset with k = 2.
```{r}
k2<-kmeans(scale(df), centers=2, nstart = 25)
fviz_cluster(k2, data=scale(df))
```
#### ii. Provide observations on the shape of the clusters you got in (b)(i) to the plot of the first two principal components in (a)(ii)
The two plots are similar. The reason behind this is that when you give kmeans a dataframe that has several columns, it does principal component analysis first to choose the best attributes.

#### iii. What is the distribution of the observations in each cluster?
```{r}
table(k2$cluster) 
```
#### iv. What is the distribution of the classes in the HTRU2 dataset?
```{r}
table(data$class) 
```
#### v. which cluster do you think corresponds to the majority class and which cluster corresponds to the minority class?
From this results, cluster 2 corresponds to the majority class (0) and cluster 1 corresponds to the minority class (1)

#### vi. Get all of the observations that belong to this cluster. Then, state what is the distribution of the classes within this large cluster; i.e., how many observations in this large cluster belong to class 1 and how many belong to class 0?
```{r}
class_0<-k2$cluster[which(data$class==0)]
class_1<-k2$cluster[which(data$class==1)]
class_0_sum<-sum(class_0[which(class_0 ==1)])
class_1_sum<-sum(class_1[which(class_1 ==1)])
cat('The number of observations that belong to this class 0 is:', class_0_sum)
cat('\nThe number of observations that belong to class 1 is:', class_1_sum)
```

#### vii. Based on the analysis above, which class (1 or 0) do you think the larger cluster represents?
The larger cluster represents class 0

#### viii. How much variance is explained by the clustering?
```{r}
variance<-k2$betweenss/k2$totss
cat('The variance that is explained by a cluster of 2 is:', variance)
```

#### ix. What is the average Silhouette width of both the clusters?
```{r}
library(fpc)
sil<-cluster::silhouette(k2$cluster, dist(scale(df)))
a<-summary(sil)
a$avg.width
```
#### x. What is the average Silhouette width of both the clusters?
```{r}
cat('The average width for cluster 1 is:', a$clus.avg.widths[1])
cat('\nThe average width for cluster 2 is:', a$clus.avg.widths[2])
```
Based on this, cluster 1 is better since the width value is higher than the average and closer to 1.

### c) Perform K-means on the result of the PCA you ran in (a).
```{r}
k3<-kmeans(pca$x[, 1:2], centers=2, nstart = 25)
```

#### i. Plot the clusters and comment on their shape with respect to the plots of a(ii) and b(i).
```{r}
fviz_cluster(k3, data=pca$x[,1:2], main="Cluster for the PCA")
```
The shape is similar to that obtained in the previous exercises

#### ii. What is the average Silhouette width of both the clusters?
```{r}
sil2<-cluster::silhouette(k3$cluster, dist(pca$x[,1:2]))
b<-summary(sil2)
b$avg.width
```

#### iii. What is the per cluster Silhouette width? Based on this, which cluster is good?
```{r}
cat('The average width for cluster 1 is:', b$clus.avg.widths[1])
cat('\nThe average width for cluster 2 is:', b$clus.avg.widths[2])
```
In this case, cluster 1 is better since the average width is higher.

#### iv.How do the values of c(ii) and c(iii) compare with those of b(ix) and b(x), respectively?
For cluster 1 the value has improved a bit.
For cluster 2 the improvement is more noticeable.
